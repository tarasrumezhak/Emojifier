{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('emoji_ukr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§—Ä–∞–Ω—Ü—É–∑—å–∫–∏–π –º–∞–∫–∞—Ä–æ–Ω —Ç–∞–∫–∏–π —Å–º–∞—á–Ω–∏–π üç¥\n",
      "—Ä–æ–±–æ—Ç–∞ –∂–∞—Ö–ª–∏–≤–∞ üòû\n",
      "—è –∑–∞—Å–º—É—á–µ–Ω–∏–π üòû\n",
      "–∫–∏–Ω—É—Ç–∏ –º'—è—á ‚öæ\n",
      "–ì–∞—Ä–Ω–∏–π –∂–∞—Ä—Ç üòÑ\n",
      "—è–∫–∞ –≤–∞—à–∞ —É–ª—é–±–ª–µ–Ω–∞ –≥—Ä–∞ –≤ –±–µ–π—Å–±–æ–ª ‚öæ\n",
      "–Ø –≥–æ—Ç—É–≤–∞–ª–∞ –º‚Äô—è—Å–æ üç¥\n",
      "–ø—Ä–∏–ø–∏–Ω—ñ—Ç—å –≤–æ–∑–∏—Ç–∏—Å—è üòû\n",
      "–Ø —Ö–æ—á—É –∫–∏—Ç–∞–π—Å—å–∫—É —ó–∂—É üç¥\n",
      "–î–∞–≤–∞–π—Ç–µ –ø—ñ–¥–µ–º–æ –≥—Ä–∞—Ç–∏ –≤ –±–µ–π—Å–±–æ–ª ‚öæ\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence '–≤—ñ–Ω –∑—Ä–æ–±–∏–≤ –¥–∏–≤–æ–≤–∏–∂–Ω—É —Ä–æ–±–æ—Ç—É' has label index 2, which is emoji üòÑ\n",
      "Label index 2 in one-hot encoding format is [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "idx = 50\n",
    "print(f\"Sentence '{X_train[idx]}' has label index {Y_train[idx]}, which is emoji {label_to_emoji(Y_train[idx])}\", )\n",
    "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('fiction.lowercased.tokenized.glove.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of –±–∞—Ç—å–∫—ñ–≤—â–∏–Ω–∞ in the vocabulary is 2733\n",
      "the 2898th word in the vocabulary is –±–µ–∑–±–æ–∂–Ω–æ\n"
     ]
    }
   ],
   "source": [
    "word = \"–±–∞—Ç—å–∫—ñ–≤—â–∏–Ω–∞\"\n",
    "idx = 2898\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "    words = sentence.lower().split()\n",
    "\n",
    "    avg = np.zeros(word_to_vec_map['–ø—Ä–∏–≤—ñ—Ç'].shape)\n",
    "    \n",
    "    total = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            total += word_to_vec_map[w]\n",
    "        except:\n",
    "            total += avg\n",
    "    avg = total / len(words)\n",
    "    \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-6.8779000e-02  1.9953000e-02 -2.9009950e-01  8.7509500e-02\n",
      "  1.4155000e-02  2.7551550e-01  7.7740250e-02  6.6722750e-02\n",
      "  1.4309525e-01 -1.6189925e-01 -3.7249525e-01 -1.2387125e-01\n",
      " -1.7837375e-01  1.0524475e+00  2.8254650e-01 -7.0859250e-02\n",
      " -1.0817500e-02 -2.9021650e-01  3.7671000e-02  2.6803300e-01\n",
      " -1.4620375e-01  2.6981500e-01 -1.7425450e-01 -1.5196750e-01\n",
      " -2.6725900e-01  3.0173000e-02 -3.8939900e-01 -8.9320000e-03\n",
      " -4.0813500e-01 -1.4401500e-01  3.5114375e-01 -3.3192025e-01\n",
      " -4.5284275e-01  2.2350500e-01  2.5134125e-01  3.1897275e-01\n",
      " -1.1154250e-01 -2.9278350e-01  1.1589250e-02  5.4871825e-01\n",
      " -6.3967175e-01  1.6656225e-01 -3.6987075e-01  1.6039700e-01\n",
      "  1.9606150e-01 -3.0023325e-01  7.7144250e-02 -6.4164000e-02\n",
      "  3.8528000e-02  8.9644750e-02 -1.9821750e-02  2.6080975e-01\n",
      " -9.0121000e-02  2.7288600e-01 -6.8264300e-01  1.1704150e-01\n",
      "  2.0684475e-01 -3.4356425e-01  7.0288750e-02 -5.5021000e-02\n",
      " -3.6088750e-02 -6.4270750e-02 -2.2802775e-01  1.6983375e-01\n",
      "  1.1958700e-01  2.2396000e-01  4.3429000e-02 -1.6344975e-01\n",
      "  1.3530475e-01  2.3357825e-01 -1.9562025e-01 -3.0680425e-01\n",
      "  1.2053400e-01  3.8123475e-01  1.7452200e-01 -5.0615000e-02\n",
      "  8.8919000e-02 -1.8570100e-01  5.4413250e-02 -2.8393725e-01\n",
      " -7.2060000e-03 -2.8040750e-02 -7.4451750e-02  6.6352000e-02\n",
      "  1.8402575e-01  1.2189650e-01  2.3697500e-03  4.3332500e-03\n",
      " -7.4253500e-02 -3.6171800e-01  2.7336350e-01 -4.3760000e-03\n",
      " -1.1881975e-01 -3.1117575e-01 -2.5625200e-01 -5.8987000e-02\n",
      "  1.0823600e-01  3.4822500e-03 -2.9050675e-01  2.3827250e-02\n",
      " -1.3709350e-01 -6.8729500e-02  7.1959250e-02  5.1831175e-01\n",
      " -2.2411000e-01  1.7636925e-01 -4.7181500e-02  4.0144500e-02\n",
      "  5.2975500e-02  4.1101575e-01 -1.6313375e-01 -2.4651525e-01\n",
      " -2.2219650e-01 -8.9800000e-04 -6.4750250e-02  1.1357250e-01\n",
      " -7.8309750e-02 -5.2586750e-02 -1.9020125e-01  7.0552950e-01\n",
      " -2.0025025e-01  1.7695000e-03 -1.1201675e-01  1.9395150e-01\n",
      " -2.5149650e-01 -5.1275250e-02 -6.8010000e-02 -6.4729750e-02\n",
      " -6.0160250e-02 -3.6308200e-01 -6.8375500e-02  4.3087475e-01\n",
      " -3.9362875e-01  1.1317775e-01 -1.3885600e-01  1.6140500e-02\n",
      "  6.9849500e-02  2.7571225e-01 -1.7239000e-02  2.9015100e-01\n",
      "  2.1421750e-01  2.8396100e-01 -2.9667475e-01  1.8794225e-01\n",
      "  9.6882500e-02 -2.8834825e-01 -6.2575250e-02 -3.6916800e-01\n",
      " -1.2260225e-01 -2.8195325e-01  1.2453775e-01  2.3795000e-03\n",
      "  4.1648500e-02  9.9891525e-01  1.3991250e-02 -4.4592575e-01\n",
      "  3.6077625e-01 -1.2253375e-01  1.0179275e-01  2.3397225e-01\n",
      "  4.4966250e-02 -3.3870925e-01  7.9620500e-02  5.2571200e-01\n",
      " -9.0057000e-02 -1.2864950e-01 -1.0280750e-01 -3.6327950e-01\n",
      "  2.8613900e-01 -2.2899175e-01  5.0074750e-02 -1.3841850e-01\n",
      "  1.1718000e-02 -1.0889900e-01  2.3765875e-01  1.8632550e-01\n",
      "  2.5441700e-01  6.7437750e-02 -4.7075950e-01 -3.6828750e-01\n",
      "  9.9131250e-02 -4.1723500e-02  3.7573525e-01 -2.6703500e-02\n",
      " -5.2350425e-01 -2.4616500e-02  2.4166575e-01  1.3536050e-01\n",
      " -1.3527600e-01 -2.3533900e-01 -2.5061425e-01  1.4444850e-01\n",
      "  4.1058975e-01 -2.9343825e-01  2.9737675e-01 -3.6163200e-01\n",
      "  6.5976750e-02 -3.8476825e-01 -1.1498925e-01  1.9009875e-01\n",
      "  2.0989675e-01 -6.1045000e-03  9.7330250e-02  1.7930250e-02\n",
      "  3.6892250e-02  3.3841850e-01 -3.7743700e-01 -1.9094500e-01\n",
      "  3.7971025e-01 -2.8389000e-01  2.5301900e-01 -2.4426625e-01\n",
      " -1.7694625e-01  7.9959250e-02 -2.7065050e-01 -3.1009750e-02\n",
      " -3.2711000e-02  7.8078750e-02 -2.8960000e-03  2.6711675e-01\n",
      "  1.1299100e-01 -1.6204875e-01  3.8831500e-01 -1.6943825e-01\n",
      " -3.2972625e-01 -7.2256750e-02  1.4168525e-01  2.7962875e-01\n",
      "  2.1976525e-01  9.6696000e-02 -1.7215200e-01  2.3643250e-01\n",
      "  3.2784750e-02 -3.7237300e-01  1.3807275e-01  4.4763850e-01\n",
      "  1.2667200e-01  2.0485425e-01 -5.4428500e-02  1.5193725e-01\n",
      " -5.6320375e-01 -1.2810725e-01  5.0659500e-02 -8.9285000e-02\n",
      " -2.8632500e-03 -2.5229850e-01 -2.1840000e-01  2.1331500e-02\n",
      " -4.1148100e-01  1.1121050e-01  5.0097400e-01 -6.8619225e-01\n",
      " -1.2584900e-01  2.8015600e-01  1.6626525e-01  4.3430450e-01\n",
      "  5.9023275e-01  5.0112000e-01  8.3742750e-02 -3.1964225e-01\n",
      "  1.3921425e-01  1.8932225e-01 -4.4711925e-01  4.1191925e-01\n",
      "  1.6328600e-01  1.0340525e-01 -2.1429175e-01  1.1465425e-01\n",
      "  1.3037250e-02 -2.1779600e-01 -7.3777900e-01  1.5491500e-01\n",
      " -3.5922550e-01  2.5829575e-01  1.8828000e-01 -1.9062275e-01\n",
      "  3.1852000e-01 -2.1719850e-01 -2.5133575e-01 -7.4296975e-01\n",
      " -2.3798275e-01  1.8726800e-01  4.2252825e-01 -5.0441700e-01\n",
      " -1.2697650e-01  3.5194300e-01  1.9477225e-01 -4.5236850e-01\n",
      " -1.5763775e-01 -2.2226650e-01 -5.0870700e-01  7.3409250e-02\n",
      " -9.6133500e-02 -1.2991425e-01  3.3681275e-01 -4.6538175e-01\n",
      " -2.4890025e-01 -2.1613675e-01 -4.7044350e-01 -1.3196225e-01]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"–Ø –ª—é–±–ª—é —Å–≤–æ—é –Ω–µ–Ω—å–∫—É\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "        \n",
    "    np.random.seed(1)\n",
    "\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 300                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    for t in range(num_iterations): \n",
    "        for i in range(m):         \n",
    "            \n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            cost = -np.sum(Y_oh[i] * np.log(a))\n",
    "            \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.1038890515158137\n",
      "Accuracy: 0.3989071038251366\n",
      "Epoch: 100 --- cost = 0.40175629021942416\n",
      "Accuracy: 0.9617486338797814\n",
      "Epoch: 200 --- cost = 0.2621279296696463\n",
      "Accuracy: 0.9836065573770492\n",
      "Epoch: 300 --- cost = 0.18762609102630362\n",
      "Accuracy: 0.9890710382513661\n",
      "[[4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.994535519125683\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "\n",
      "–∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–º—ñ—à–Ω–æ üòÑ\n",
      "—è —Ö–æ—á—É —ó—Å—Ç–∏ üç¥\n",
      "–≤—ñ–Ω –∫–æ—Ö–∞—î —ó—ó ‚ù§Ô∏è\n",
      "–æ—Ü–µ –≤–µ—Å–µ–ª–∏–π –∂–∞—Ä—Ç üòÑ\n",
      "–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ —Ñ—É—Ç–±–æ–ª ‚öæ\n",
      "—Ç–µ–ø–µ—Ä —è —Å—É–º–Ω–∏–π üòû\n",
      "–¢–∞—Ä–∞—Å —Å–º—ñ—à–Ω–∏–π —Ö–ª–æ–ø—á–∏–Ω–∞ üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"–∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–º—ñ—à–Ω–æ\", \"—è —Ö–æ—á—É —ó—Å—Ç–∏\", \"–≤—ñ–Ω –∫–æ—Ö–∞—î —ó—ó\", \"–æ—Ü–µ –≤–µ—Å–µ–ª–∏–π –∂–∞—Ä—Ç\", \"–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ —Ñ—É—Ç–±–æ–ª\", \"—Ç–µ–ø–µ—Ä —è —Å—É–º–Ω–∏–π\", \"–¢–∞—Ä–∞—Å —Å–º—ñ—à–Ω–∏–π —Ö–ª–æ–ø—á–∏–Ω–∞\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-8cb91172c208>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mActivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEmbedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRandomRotation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     raise ImportError(\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;34m'Keras requires TensorFlow 2.2 or higher. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         'Install TensorFlow via `pip install tensorflow`')\n",
      "\u001b[0;31mImportError\u001b[0m: Keras requires TensorFlow 2.2 or higher. Install TensorFlow via `pip install tensorflow`"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
