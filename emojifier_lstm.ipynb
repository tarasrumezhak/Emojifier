{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emoji\n",
      "  Downloading emoji-0.6.0.tar.gz (51 kB)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py): started\n",
      "  Building wheel for emoji (setup.py): finished with status 'done'\n",
      "  Created wheel for emoji: filename=emoji-0.6.0-py3-none-any.whl size=49720 sha256=36ffa5100f5e2b0618a69c3976d9acb553480ac6300567feab6f2b16f873ae82\n",
      "  Stored in directory: c:\\users\\user\\appdata\\local\\pip\\cache\\wheels\\4e\\bf\\6b\\2e22b3708d14bf6384f862db539b044d6931bd6b14ad3c9adc\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install emoji "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from emo_utils import *\n",
    "import emoji\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('emoji_ukr.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–§—Ä–∞–Ω—Ü—É–∑—å–∫–∏–π –º–∞–∫–∞—Ä–æ–Ω —Ç–∞–∫–∏–π —Å–º–∞—á–Ω–∏–π üç¥\n",
      "—Ä–æ–±–æ—Ç–∞ –∂–∞—Ö–ª–∏–≤–∞ üòû\n",
      "—è –∑–∞—Å–º—É—á–µ–Ω–∏–π üòû\n",
      "–∫–∏–Ω—É—Ç–∏ –º'—è—á ‚öæ\n",
      "–ì–∞—Ä–Ω–∏–π –∂–∞—Ä—Ç üòÑ\n",
      "—è–∫–∞ –≤–∞—à–∞ —É–ª—é–±–ª–µ–Ω–∞ –≥—Ä–∞ –≤ –±–µ–π—Å–±–æ–ª ‚öæ\n",
      "–Ø –≥–æ—Ç—É–≤–∞–ª–∞ –º‚Äô—è—Å–æ üç¥\n",
      "–ø—Ä–∏–ø–∏–Ω—ñ—Ç—å –≤–æ–∑–∏—Ç–∏—Å—è üòû\n",
      "–Ø —Ö–æ—á—É –∫–∏—Ç–∞–π—Å—å–∫—É —ó–∂—É üç¥\n",
      "–î–∞–≤–∞–π—Ç–µ –ø—ñ–¥–µ–º–æ –≥—Ä–∞—Ç–∏ –≤ –±–µ–π—Å–±–æ–ª ‚öæ\n"
     ]
    }
   ],
   "source": [
    "for idx in range(10):\n",
    "    print(X_train[idx], label_to_emoji(Y_train[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence '–≤—ñ–Ω –∑—Ä–æ–±–∏–≤ –¥–∏–≤–æ–≤–∏–∂–Ω—É —Ä–æ–±–æ—Ç—É' has label index 2, which is emoji üòÑ\n",
      "Label index 2 in one-hot encoding format is [0. 0. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "idx = 50\n",
    "print(f\"Sentence '{X_train[idx]}' has label index {Y_train[idx]}, which is emoji {label_to_emoji(Y_train[idx])}\", )\n",
    "print(f\"Label index {Y_train[idx]} in one-hot encoding format is {Y_oh_train[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('fiction.lowercased.tokenized.glove.300d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the index of –±–∞—Ç—å–∫—ñ–≤—â–∏–Ω–∞ in the vocabulary is 2733\n",
      "the 2898th word in the vocabulary is –±–µ–∑–±–æ–∂–Ω–æ\n"
     ]
    }
   ],
   "source": [
    "word = \"–±–∞—Ç—å–∫—ñ–≤—â–∏–Ω–∞\"\n",
    "idx = 2898\n",
    "print(\"the index of\", word, \"in the vocabulary is\", word_to_index[word])\n",
    "print(\"the\", str(idx) + \"th word in the vocabulary is\", index_to_word[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_avg(sentence, word_to_vec_map):\n",
    "\n",
    "    words = sentence.lower().split()\n",
    "    avg = np.zeros(word_to_vec_map['–ø—Ä–∏–≤—ñ—Ç'].shape)\n",
    "    \n",
    "    total = 0\n",
    "    for w in words:\n",
    "        try:\n",
    "            total += word_to_vec_map[w]\n",
    "        except:\n",
    "            total += avg\n",
    "    avg = total / len(words)\n",
    "        \n",
    "    return avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg = \n",
      " [-6.8779000e-02  1.9953000e-02 -2.9009950e-01  8.7509500e-02\n",
      "  1.4155000e-02  2.7551550e-01  7.7740250e-02  6.6722750e-02\n",
      "  1.4309525e-01 -1.6189925e-01 -3.7249525e-01 -1.2387125e-01\n",
      " -1.7837375e-01  1.0524475e+00  2.8254650e-01 -7.0859250e-02\n",
      " -1.0817500e-02 -2.9021650e-01  3.7671000e-02  2.6803300e-01\n",
      " -1.4620375e-01  2.6981500e-01 -1.7425450e-01 -1.5196750e-01\n",
      " -2.6725900e-01  3.0173000e-02 -3.8939900e-01 -8.9320000e-03\n",
      " -4.0813500e-01 -1.4401500e-01  3.5114375e-01 -3.3192025e-01\n",
      " -4.5284275e-01  2.2350500e-01  2.5134125e-01  3.1897275e-01\n",
      " -1.1154250e-01 -2.9278350e-01  1.1589250e-02  5.4871825e-01\n",
      " -6.3967175e-01  1.6656225e-01 -3.6987075e-01  1.6039700e-01\n",
      "  1.9606150e-01 -3.0023325e-01  7.7144250e-02 -6.4164000e-02\n",
      "  3.8528000e-02  8.9644750e-02 -1.9821750e-02  2.6080975e-01\n",
      " -9.0121000e-02  2.7288600e-01 -6.8264300e-01  1.1704150e-01\n",
      "  2.0684475e-01 -3.4356425e-01  7.0288750e-02 -5.5021000e-02\n",
      " -3.6088750e-02 -6.4270750e-02 -2.2802775e-01  1.6983375e-01\n",
      "  1.1958700e-01  2.2396000e-01  4.3429000e-02 -1.6344975e-01\n",
      "  1.3530475e-01  2.3357825e-01 -1.9562025e-01 -3.0680425e-01\n",
      "  1.2053400e-01  3.8123475e-01  1.7452200e-01 -5.0615000e-02\n",
      "  8.8919000e-02 -1.8570100e-01  5.4413250e-02 -2.8393725e-01\n",
      " -7.2060000e-03 -2.8040750e-02 -7.4451750e-02  6.6352000e-02\n",
      "  1.8402575e-01  1.2189650e-01  2.3697500e-03  4.3332500e-03\n",
      " -7.4253500e-02 -3.6171800e-01  2.7336350e-01 -4.3760000e-03\n",
      " -1.1881975e-01 -3.1117575e-01 -2.5625200e-01 -5.8987000e-02\n",
      "  1.0823600e-01  3.4822500e-03 -2.9050675e-01  2.3827250e-02\n",
      " -1.3709350e-01 -6.8729500e-02  7.1959250e-02  5.1831175e-01\n",
      " -2.2411000e-01  1.7636925e-01 -4.7181500e-02  4.0144500e-02\n",
      "  5.2975500e-02  4.1101575e-01 -1.6313375e-01 -2.4651525e-01\n",
      " -2.2219650e-01 -8.9800000e-04 -6.4750250e-02  1.1357250e-01\n",
      " -7.8309750e-02 -5.2586750e-02 -1.9020125e-01  7.0552950e-01\n",
      " -2.0025025e-01  1.7695000e-03 -1.1201675e-01  1.9395150e-01\n",
      " -2.5149650e-01 -5.1275250e-02 -6.8010000e-02 -6.4729750e-02\n",
      " -6.0160250e-02 -3.6308200e-01 -6.8375500e-02  4.3087475e-01\n",
      " -3.9362875e-01  1.1317775e-01 -1.3885600e-01  1.6140500e-02\n",
      "  6.9849500e-02  2.7571225e-01 -1.7239000e-02  2.9015100e-01\n",
      "  2.1421750e-01  2.8396100e-01 -2.9667475e-01  1.8794225e-01\n",
      "  9.6882500e-02 -2.8834825e-01 -6.2575250e-02 -3.6916800e-01\n",
      " -1.2260225e-01 -2.8195325e-01  1.2453775e-01  2.3795000e-03\n",
      "  4.1648500e-02  9.9891525e-01  1.3991250e-02 -4.4592575e-01\n",
      "  3.6077625e-01 -1.2253375e-01  1.0179275e-01  2.3397225e-01\n",
      "  4.4966250e-02 -3.3870925e-01  7.9620500e-02  5.2571200e-01\n",
      " -9.0057000e-02 -1.2864950e-01 -1.0280750e-01 -3.6327950e-01\n",
      "  2.8613900e-01 -2.2899175e-01  5.0074750e-02 -1.3841850e-01\n",
      "  1.1718000e-02 -1.0889900e-01  2.3765875e-01  1.8632550e-01\n",
      "  2.5441700e-01  6.7437750e-02 -4.7075950e-01 -3.6828750e-01\n",
      "  9.9131250e-02 -4.1723500e-02  3.7573525e-01 -2.6703500e-02\n",
      " -5.2350425e-01 -2.4616500e-02  2.4166575e-01  1.3536050e-01\n",
      " -1.3527600e-01 -2.3533900e-01 -2.5061425e-01  1.4444850e-01\n",
      "  4.1058975e-01 -2.9343825e-01  2.9737675e-01 -3.6163200e-01\n",
      "  6.5976750e-02 -3.8476825e-01 -1.1498925e-01  1.9009875e-01\n",
      "  2.0989675e-01 -6.1045000e-03  9.7330250e-02  1.7930250e-02\n",
      "  3.6892250e-02  3.3841850e-01 -3.7743700e-01 -1.9094500e-01\n",
      "  3.7971025e-01 -2.8389000e-01  2.5301900e-01 -2.4426625e-01\n",
      " -1.7694625e-01  7.9959250e-02 -2.7065050e-01 -3.1009750e-02\n",
      " -3.2711000e-02  7.8078750e-02 -2.8960000e-03  2.6711675e-01\n",
      "  1.1299100e-01 -1.6204875e-01  3.8831500e-01 -1.6943825e-01\n",
      " -3.2972625e-01 -7.2256750e-02  1.4168525e-01  2.7962875e-01\n",
      "  2.1976525e-01  9.6696000e-02 -1.7215200e-01  2.3643250e-01\n",
      "  3.2784750e-02 -3.7237300e-01  1.3807275e-01  4.4763850e-01\n",
      "  1.2667200e-01  2.0485425e-01 -5.4428500e-02  1.5193725e-01\n",
      " -5.6320375e-01 -1.2810725e-01  5.0659500e-02 -8.9285000e-02\n",
      " -2.8632500e-03 -2.5229850e-01 -2.1840000e-01  2.1331500e-02\n",
      " -4.1148100e-01  1.1121050e-01  5.0097400e-01 -6.8619225e-01\n",
      " -1.2584900e-01  2.8015600e-01  1.6626525e-01  4.3430450e-01\n",
      "  5.9023275e-01  5.0112000e-01  8.3742750e-02 -3.1964225e-01\n",
      "  1.3921425e-01  1.8932225e-01 -4.4711925e-01  4.1191925e-01\n",
      "  1.6328600e-01  1.0340525e-01 -2.1429175e-01  1.1465425e-01\n",
      "  1.3037250e-02 -2.1779600e-01 -7.3777900e-01  1.5491500e-01\n",
      " -3.5922550e-01  2.5829575e-01  1.8828000e-01 -1.9062275e-01\n",
      "  3.1852000e-01 -2.1719850e-01 -2.5133575e-01 -7.4296975e-01\n",
      " -2.3798275e-01  1.8726800e-01  4.2252825e-01 -5.0441700e-01\n",
      " -1.2697650e-01  3.5194300e-01  1.9477225e-01 -4.5236850e-01\n",
      " -1.5763775e-01 -2.2226650e-01 -5.0870700e-01  7.3409250e-02\n",
      " -9.6133500e-02 -1.2991425e-01  3.3681275e-01 -4.6538175e-01\n",
      " -2.4890025e-01 -2.1613675e-01 -4.7044350e-01 -1.3196225e-01]\n"
     ]
    }
   ],
   "source": [
    "avg = sentence_to_avg(\"–Ø –ª—é–±–ª—é —Å–≤–æ—é –Ω–µ–Ω—å–∫—É\", word_to_vec_map)\n",
    "print(\"avg = \\n\", avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, word_to_vec_map, learning_rate = 0.01, num_iterations = 400):\n",
    "    \n",
    "    np.random.seed(1)\n",
    "\n",
    "    m = Y.shape[0]                          # number of training examples\n",
    "    n_y = 5                                 # number of classes  \n",
    "    n_h = 300                                # dimensions of the GloVe vectors \n",
    "    \n",
    "    W = np.random.randn(n_y, n_h) / np.sqrt(n_h)\n",
    "    b = np.zeros((n_y,))\n",
    "    \n",
    "    Y_oh = convert_to_one_hot(Y, C = n_y) \n",
    "    \n",
    "    for t in range(num_iterations): \n",
    "        for i in range(m):         \n",
    "            \n",
    "            avg = sentence_to_avg(X[i], word_to_vec_map)\n",
    "\n",
    "           \n",
    "            z = np.dot(W, avg) + b\n",
    "            a = softmax(z)\n",
    "\n",
    "            cost = -np.sum(Y_oh[i] * np.log(a))\n",
    "            \n",
    "            # Compute gradients \n",
    "            dz = a - Y_oh[i]\n",
    "            dW = np.dot(dz.reshape(n_y,1), avg.reshape(1, n_h))\n",
    "            db = dz\n",
    "\n",
    "            W = W - learning_rate * dW\n",
    "            b = b - learning_rate * db\n",
    "        \n",
    "        if t % 100 == 0:\n",
    "            print(\"Epoch: \" + str(t) + \" --- cost = \" + str(cost))\n",
    "            pred = predict(X, Y, W, b, word_to_vec_map) #predict is defined in emo_utils.py\n",
    "\n",
    "    return pred, W, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 --- cost = 1.1038890515158137\n",
      "Accuracy: 0.3989071038251366\n",
      "Epoch: 100 --- cost = 0.40175629021942433\n",
      "Accuracy: 0.9617486338797814\n",
      "Epoch: 200 --- cost = 0.26212792966964615\n",
      "Accuracy: 0.9836065573770492\n",
      "Epoch: 300 --- cost = 0.18762609102630362\n",
      "Accuracy: 0.9890710382513661\n",
      "[[4.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [1.]\n",
      " [4.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [4.]\n",
      " [0.]\n",
      " [2.]\n",
      " [1.]\n",
      " [3.]\n",
      " [1.]\n",
      " [0.]\n",
      " [4.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [1.]\n",
      " [3.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [4.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [4.]\n",
      " [4.]\n",
      " [2.]\n",
      " [2.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [0.]\n",
      " [2.]\n",
      " [0.]\n",
      " [0.]\n",
      " [4.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [4.]\n",
      " [2.]\n",
      " [1.]\n",
      " [2.]\n",
      " [4.]\n",
      " [3.]\n",
      " [1.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [2.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [2.]\n",
      " [4.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [3.]\n",
      " [0.]\n",
      " [0.]\n",
      " [1.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [2.]\n",
      " [0.]\n",
      " [3.]\n",
      " [2.]\n",
      " [3.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]\n",
      " [2.]\n",
      " [4.]\n",
      " [2.]\n",
      " [3.]\n",
      " [1.]\n",
      " [2.]]\n"
     ]
    }
   ],
   "source": [
    "pred, W, b = model(X_train, Y_train, word_to_vec_map)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set:\n",
      "Accuracy: 0.994535519125683\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set:\")\n",
    "pred_train = predict(X_train, Y_train, W, b, word_to_vec_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.0\n",
      "\n",
      "–∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–º—ñ—à–Ω–æ üòÑ\n",
      "—è —Ö–æ—á—É —ó—Å—Ç–∏ üç¥\n",
      "–≤—ñ–Ω –∫–æ—Ö–∞—î —ó—ó ‚ù§Ô∏è\n",
      "–æ—Ü–µ –≤–µ—Å–µ–ª–∏–π –∂–∞—Ä—Ç üòÑ\n",
      "–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ —Ñ—É—Ç–±–æ–ª ‚öæ\n",
      "—Ç–µ–ø–µ—Ä —è —Å—É–º–Ω–∏–π üòû\n",
      "–¢–∞—Ä–∞—Å —Å–º—ñ—à–Ω–∏–π —Ö–ª–æ–ø—á–∏–Ω–∞ üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"–∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–º—ñ—à–Ω–æ\", \"—è —Ö–æ—á—É —ó—Å—Ç–∏\", \"–≤—ñ–Ω –∫–æ—Ö–∞—î —ó—ó\", \"–æ—Ü–µ –≤–µ—Å–µ–ª–∏–π –∂–∞—Ä—Ç\", \"–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ —Ñ—É—Ç–±–æ–ª\", \"—Ç–µ–ø–µ—Ä —è —Å—É–º–Ω–∏–π\", \"–¢–∞—Ä–∞—Å —Å–º—ñ—à–Ω–∏–π —Ö–ª–æ–ø—á–∏–Ω–∞\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "pred = predict(X_my_sentences, Y_my_labels , W, b, word_to_vec_map)\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 –Ø\n",
      "1 –ª—é–±–ª—é\n",
      "2 –∫–æ–¥–∏—Ç–∏\n"
     ]
    }
   ],
   "source": [
    "for idx, val in enumerate([\"–Ø\", \"–ª—é–±–ª—é\", \"–∫–æ–¥–∏—Ç–∏\"]):\n",
    "    print(idx,val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \n",
    "    \n",
    "    m = X.shape[0]                                  \n",
    "    \n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    \n",
    "    for i in range(m):                              \n",
    "        \n",
    "        sentence_words = X[i].lower().split()\n",
    "        \n",
    "        j = 0\n",
    "        \n",
    "        for w in sentence_words:\n",
    "            try:\n",
    "                X_indices[i, j] = word_to_index[w]\n",
    "                j = j + 1\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "      \n",
    "    vocab_len = len(word_to_index) + 1                 \n",
    "    emb_dim = word_to_vec_map[\"–æ–≥—ñ—Ä–æ–∫\"].shape[0]      \n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, idx in word_to_index.items():\n",
    "        emb_matrix[idx, :] = word_to_vec_map[word]\n",
    "\n",
    "   \n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "\n",
    "    embedding_layer.build((None,)) \n",
    "    \n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 7)                 0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 7, 300)            33109500  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 7, 128)            219648    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 7, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 33,461,377\n",
      "Trainable params: 351,877\n",
      "Non-trainable params: 33,109,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "maxLen = len(max(X_train, key=len).split())\n",
    "model = Emojify_V2((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\User\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/50\n",
      "183/183 [==============================] - 1s 7ms/step - loss: 1.5779 - accuracy: 0.2350\n",
      "Epoch 2/50\n",
      "183/183 [==============================] - 0s 513us/step - loss: 1.4658 - accuracy: 0.3825\n",
      "Epoch 3/50\n",
      "183/183 [==============================] - 0s 513us/step - loss: 1.4193 - accuracy: 0.4208\n",
      "Epoch 4/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 1.3083 - accuracy: 0.5027\n",
      "Epoch 5/50\n",
      "183/183 [==============================] - 0s 546us/step - loss: 1.1785 - accuracy: 0.5792\n",
      "Epoch 6/50\n",
      "183/183 [==============================] - 0s 502us/step - loss: 1.0244 - accuracy: 0.6284\n",
      "Epoch 7/50\n",
      "183/183 [==============================] - 0s 502us/step - loss: 0.9149 - accuracy: 0.6776\n",
      "Epoch 8/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.7455 - accuracy: 0.7377\n",
      "Epoch 9/50\n",
      "183/183 [==============================] - 0s 677us/step - loss: 0.6245 - accuracy: 0.7596\n",
      "Epoch 10/50\n",
      "183/183 [==============================] - 0s 546us/step - loss: 0.5048 - accuracy: 0.7978\n",
      "Epoch 11/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.4683 - accuracy: 0.8361\n",
      "Epoch 12/50\n",
      "183/183 [==============================] - 0s 600us/step - loss: 0.3953 - accuracy: 0.8634\n",
      "Epoch 13/50\n",
      "183/183 [==============================] - 0s 601us/step - loss: 0.3097 - accuracy: 0.9071\n",
      "Epoch 14/50\n",
      "183/183 [==============================] - 0s 633us/step - loss: 0.3008 - accuracy: 0.9016\n",
      "Epoch 15/50\n",
      "183/183 [==============================] - 0s 589us/step - loss: 0.2962 - accuracy: 0.9071\n",
      "Epoch 16/50\n",
      "183/183 [==============================] - 0s 601us/step - loss: 0.2684 - accuracy: 0.9126\n",
      "Epoch 17/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.1922 - accuracy: 0.9399\n",
      "Epoch 18/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.1368 - accuracy: 0.9617\n",
      "Epoch 19/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.0972 - accuracy: 0.9781\n",
      "Epoch 20/50\n",
      "183/183 [==============================] - 0s 589us/step - loss: 0.0765 - accuracy: 0.9781\n",
      "Epoch 21/50\n",
      "183/183 [==============================] - 0s 589us/step - loss: 0.0568 - accuracy: 0.9836\n",
      "Epoch 22/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.0435 - accuracy: 0.9836\n",
      "Epoch 23/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.0398 - accuracy: 0.9836\n",
      "Epoch 24/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.0345 - accuracy: 0.9836\n",
      "Epoch 25/50\n",
      "183/183 [==============================] - 0s 601us/step - loss: 0.0328 - accuracy: 0.9891\n",
      "Epoch 26/50\n",
      "183/183 [==============================] - 0s 568us/step - loss: 0.0260 - accuracy: 0.9891\n",
      "Epoch 27/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.0243 - accuracy: 0.9891\n",
      "Epoch 28/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.0211 - accuracy: 0.9945\n",
      "Epoch 29/50\n",
      "183/183 [==============================] - 0s 546us/step - loss: 0.0203 - accuracy: 0.9945\n",
      "Epoch 30/50\n",
      "183/183 [==============================] - 0s 546us/step - loss: 0.0204 - accuracy: 0.9891\n",
      "Epoch 31/50\n",
      "183/183 [==============================] - 0s 546us/step - loss: 0.0232 - accuracy: 0.9891\n",
      "Epoch 32/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.0178 - accuracy: 0.9891\n",
      "Epoch 33/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.0168 - accuracy: 0.9891\n",
      "Epoch 34/50\n",
      "183/183 [==============================] - 0s 546us/step - loss: 0.0146 - accuracy: 0.9945\n",
      "Epoch 35/50\n",
      "183/183 [==============================] - 0s 546us/step - loss: 0.0110 - accuracy: 0.9945\n",
      "Epoch 36/50\n",
      "183/183 [==============================] - 0s 546us/step - loss: 0.0107 - accuracy: 0.9945\n",
      "Epoch 37/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.0075 - accuracy: 1.0000\n",
      "Epoch 38/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.0091 - accuracy: 0.9945\n",
      "Epoch 39/50\n",
      "183/183 [==============================] - 0s 524us/step - loss: 0.0055 - accuracy: 1.0000\n",
      "Epoch 40/50\n",
      "183/183 [==============================] - 0s 568us/step - loss: 0.0058 - accuracy: 1.0000\n",
      "Epoch 41/50\n",
      "183/183 [==============================] - 0s 589us/step - loss: 0.0064 - accuracy: 1.0000\n",
      "Epoch 42/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.0065 - accuracy: 1.0000\n",
      "Epoch 43/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 44/50\n",
      "183/183 [==============================] - 0s 589us/step - loss: 0.0040 - accuracy: 1.0000\n",
      "Epoch 45/50\n",
      "183/183 [==============================] - 0s 589us/step - loss: 0.0056 - accuracy: 1.0000\n",
      "Epoch 46/50\n",
      "183/183 [==============================] - 0s 633us/step - loss: 0.0037 - accuracy: 1.0000\n",
      "Epoch 47/50\n",
      "183/183 [==============================] - 0s 633us/step - loss: 0.0045 - accuracy: 1.0000\n",
      "Epoch 48/50\n",
      "183/183 [==============================] - 0s 611us/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 49/50\n",
      "183/183 [==============================] - 0s 589us/step - loss: 0.0030 - accuracy: 1.0000\n",
      "Epoch 50/50\n",
      "183/183 [==============================] - 0s 589us/step - loss: 0.0029 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x2628858ad30>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 50, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-f4ff52f764ad>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX_test_indices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msentences_to_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_len\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxLen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mY_test_oh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_one_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test_oh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Test accuracy = \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0macc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–º—ñ—à–Ω–æ üòÑ\n",
      "—è —Ö–æ—á—É —ó—Å—Ç–∏ üç¥\n",
      "–≤—ñ–Ω –∫–æ—Ö–∞—î —ó—ó ‚ù§Ô∏è\n",
      "–æ—Ü–µ –≤–µ—Å–µ–ª–∏–π –∂–∞—Ä—Ç üòÑ\n",
      "–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ —Ñ—É—Ç–±–æ–ª ‚öæ\n",
      "—Ç–µ–ø–µ—Ä —è —Å—É–º–Ω–∏–π üòû\n",
      "–¢–∞—Ä–∞—Å —Å–º—ñ—à–Ω–∏–π —Ö–ª–æ–ø—á–∏–Ω–∞ üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"–∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–º—ñ—à–Ω–æ\", \"—è —Ö–æ—á—É —ó—Å—Ç–∏\", \"–≤—ñ–Ω –∫–æ—Ö–∞—î —ó—ó\", \"–æ—Ü–µ –≤–µ—Å–µ–ª–∏–π –∂–∞—Ä—Ç\", \"–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ —Ñ—É—Ç–±–æ–ª\", \"—Ç–µ–ø–µ—Ä —è —Å—É–º–Ω–∏–π\", \"–¢–∞—Ä–∞—Å —Å–º—ñ—à–Ω–∏–π —Ö–ª–æ–ø—á–∏–Ω–∞\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–º—ñ—à–Ω–æ üòÑ\n",
      "—è —Ö–æ—á—É —ó—Å—Ç–∏ üç¥\n",
      "–≤—ñ–Ω –∫–æ—Ö–∞—î —ó—ó ‚ù§Ô∏è\n",
      "–æ—Ü–µ –≤–µ—Å–µ–ª–∏–π –∂–∞—Ä—Ç üòÑ\n",
      "–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ —Ñ—É—Ç–±–æ–ª ‚öæ\n",
      "—Ç–µ–ø–µ—Ä —è —Å—É–º–Ω–∏–π üòû\n",
      "–¢–∞—Ä–∞—Å —Å–º—ñ—à–Ω–∏–π —Ö–ª–æ–ø—á–∏–Ω–∞ üòÑ\n"
     ]
    }
   ],
   "source": [
    "X_my_sentences = np.array([\"–∑–≤—É—á–∏—Ç—å —Ç–∞–∫ —Å–º—ñ—à–Ω–æ\", \"—è —Ö–æ—á—É —ó—Å—Ç–∏\", \"–≤—ñ–Ω –∫–æ—Ö–∞—î —ó—ó\", \"–æ—Ü–µ –≤–µ—Å–µ–ª–∏–π –∂–∞—Ä—Ç\", \"–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ —Ñ—É—Ç–±–æ–ª\", \"—Ç–µ–ø–µ—Ä —è —Å—É–º–Ω–∏–π\", \"–¢–∞—Ä–∞—Å —Å–º—ñ—à–Ω–∏–π —Ö–ª–æ–ø—á–∏–Ω–∞\"])\n",
    "Y_my_labels = np.array([[0], [0], [2], [1], [4],[3]])\n",
    "\n",
    "print_predictions(X_my_sentences, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ –±–∞—Å–∫–µ—Ç–±–æ–ª üòÑ\n"
     ]
    }
   ],
   "source": [
    "x_test = np.array(['–ø—ñ—à–ª–∏ –≥—Ä–∞—Ç–∏ –±–∞—Å–∫–µ—Ç–±–æ–ª'])\n",
    "X_test_indices = sentences_to_indices(x_test, word_to_index, maxLen)\n",
    "print(x_test[0] +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
